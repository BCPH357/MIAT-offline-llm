# 内网 LLM 快速使用指南

## 快速开始（3 步搞定）

### 第 1 步: 启动服务

```bash
docker-compose up -d
```

### 第 2 步: 下载模型

```bash
docker exec -it ollama_service ollama pull gpt-oss:20b
```

### 第 3 步: 开始使用

```bash
# 测试是否正常
curl http://localhost:11436/api/tags
```

完成！现在你可以开始使用了。

> **注意**: 本配置使用端口 11436（避免與其他服務衝突）

---

## 内网访问配置

### 查看服务器 IP 地址

**Windows**:
```powershell
ipconfig
```
找到 IPv4 地址，例如：`192.168.1.100`

**Linux/Mac**:
```bash
ip addr show
# 或
ifconfig
```

### 开放防火墙端口

**Windows**（管理员权限运行 PowerShell）:
```powershell
New-NetFirewallRule -DisplayName "Ollama API" -Direction Inbound -LocalPort 11436 -Protocol TCP -Action Allow
```

**Linux**:
```bash
# Ubuntu/Debian
sudo ufw allow 11436/tcp

# CentOS/RHEL
sudo firewall-cmd --permanent --add-port=11436/tcp
sudo firewall-cmd --reload
```

### 从其他设备访问

假设服务器 IP 是 `192.168.1.100`

**测试连接**:
```bash
curl http://192.168.1.100:11436/api/tags
```

**Python 调用示例**:
```python
import requests

response = requests.post(
    "http://192.168.1.100:11436/api/generate",
    json={
        "model": "gpt-oss:20b",
        "prompt": "你好",
        "stream": False
    }
)

print(response.json()['response'])
```

---

## 常用 API 调用

### 1. 简单问答

```bash
curl http://localhost:11436/api/generate -d '{
  "model": "gpt-oss:20b",
  "prompt": "什么是人工智能？",
  "stream": false
}'
```

### 2. 对话模式

```bash
curl http://localhost:11436/api/chat -d '{
  "model": "gpt-oss:20b",
  "messages": [
    {"role": "user", "content": "你好"}
  ],
  "stream": false
}'
```

### 3. 列出所有模型

```bash
curl http://localhost:11436/api/tags
```

---

## Python 完整示例

```python
import requests

# 配置
SERVER_IP = "192.168.1.100"  # 替换为实际 IP
API_URL = f"http://{SERVER_IP}:11436"
MODEL = "gpt-oss:20b"

def ask_llm(question):
    """向 LLM 提问"""
    response = requests.post(
        f"{API_URL}/api/generate",
        json={
            "model": MODEL,
            "prompt": question,
            "stream": False
        },
        timeout=300
    )

    if response.status_code == 200:
        return response.json()['response']
    else:
        return f"错误: {response.status_code}"

# 使用示例
answer = ask_llm("介绍一下 Python 编程语言")
print(answer)
```

---

## 常见问题

### Q1: 其他设备访问不了怎么办？

**检查清单**:
1. 服务是否在运行？
   ```bash
   docker-compose ps
   ```

2. 防火墙是否开放端口？
   ```bash
   # Windows
   netsh advfirewall firewall show rule name="Ollama API"

   # Linux
   sudo ufw status
   ```

3. 网络是否连通？
   ```bash
   ping 192.168.1.100
   ```

### Q2: 如何查看我的内网 IP？

**Windows**:
- 按 `Win + R`，输入 `cmd`
- 输入 `ipconfig`
- 找到 "IPv4 地址"

**Linux/Mac**:
```bash
hostname -I
# 或
ip addr show | grep "inet "
```

### Q3: 如何修改端口？

编辑 `docker-compose.yml`:
```yaml
ports:
  - "你的端口:11434"  # 例如 "8080:11434"
```

然后重启服务：
```bash
docker-compose down
docker-compose up -d
```

### Q4: 如何停止服务？

```bash
docker-compose down
```

### Q5: 如何查看日志？

```bash
docker-compose logs -f ollama
```

---

## 性能优化建议

### 使用 GPU 加速

确认 GPU 是否被使用：
```bash
docker exec -it ollama_service nvidia-smi
```

### 预加载模型（加快首次响应）

```bash
docker exec -it ollama_service ollama run gpt-oss:20b "hello"
```

### 使用更小的模型（如果性能不足）

```bash
# 7B 模型更小更快
docker exec -it ollama_service ollama pull gpt-oss:7b
```

---

## 移动设备访问

### 手机浏览器测试

在手机浏览器中访问（确保手机连接到同一个 Wi-Fi）：
```
http://192.168.1.100:11436/api/tags
```

### 移动应用开发示例

**JavaScript/React Native**:
```javascript
const SERVER_IP = "192.168.1.100";

async function askLLM(question) {
  const response = await fetch(`http://${SERVER_IP}:11436/api/generate`, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'gpt-oss:20b',
      prompt: question,
      stream: false
    })
  });

  const data = await response.json();
  return data.response;
}

// 使用
askLLM("你好").then(answer => console.log(answer));
```

---

## 安全提示

1. 只在受信任的内网中使用
2. 不要将 11436 端口暴露到公网
3. 定期检查日志，监控异常访问
4. 如需公网访问，建议使用 VPN

---

## 快速命令参考

```bash
# 启动服务
docker-compose up -d

# 停止服务
docker-compose down

# 重启服务
docker-compose restart

# 查看状态
docker-compose ps

# 查看日志
docker-compose logs -f

# 进入容器
docker exec -it ollama_service bash

# 列出模型
docker exec -it ollama_service ollama list

# 下载模型
docker exec -it ollama_service ollama pull <model-name>

# 删除模型
docker exec -it ollama_service ollama rm <model-name>
```

---

## 获取帮助

- 查看完整文档: `README.md`
- 运行示例代码: `python api_examples.py`
- Ollama 官方文档: https://github.com/ollama/ollama

---

**祝使用愉快！**
