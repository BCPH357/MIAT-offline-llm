"""
Ollama API è°ƒç”¨ç¤ºä¾‹
æ”¯æŒæœ¬åœ°è°ƒç”¨å’Œé€šè¿‡ ngrok å¤–ç½‘è°ƒç”¨
"""

import requests
import json

# ===========================
# é…ç½®åŒºåŸŸ
# ===========================

# æœ¬åœ° Ollama API åœ°å€
LOCAL_API_URL = "http://localhost:11436"

# Ngrok å¤–ç½‘åœ°å€ï¼ˆå¯åŠ¨æœåŠ¡åä» http://localhost:4040 è·å–ï¼‰
# ç¤ºä¾‹: "https://xxxx-xx-xx-xxx-xxx.ngrok-free.app"
NGROK_API_URL = "YOUR_NGROK_URL_HERE"

# ä½¿ç”¨çš„æ¨¡å‹åç§°
MODEL_NAME = "gpt-oss:20b"


# ===========================
# API è°ƒç”¨å‡½æ•°
# ===========================

def call_ollama_generate(api_url, model, prompt, stream=False):
    """
    è°ƒç”¨ Ollama Generate API

    å‚æ•°:
        api_url: API åŸºç¡€åœ°å€
        model: æ¨¡å‹åç§°
        prompt: æç¤ºè¯
        stream: æ˜¯å¦ä½¿ç”¨æµå¼è¾“å‡º

    è¿”å›:
        å“åº”å†…å®¹
    """
    endpoint = f"{api_url}/api/generate"

    payload = {
        "model": model,
        "prompt": prompt,
        "stream": stream
    }

    try:
        print(f"ğŸš€ æ­£åœ¨è°ƒç”¨ API: {endpoint}")
        print(f"ğŸ“ æç¤ºè¯: {prompt}")
        print("-" * 50)

        response = requests.post(
            endpoint,
            json=payload,
            timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
        )

        response.raise_for_status()

        if stream:
            # æµå¼è¾“å‡º
            print("ğŸ“¡ æµå¼å“åº”:")
            full_response = ""
            for line in response.iter_lines():
                if line:
                    data = json.loads(line)
                    if 'response' in data:
                        chunk = data['response']
                        print(chunk, end='', flush=True)
                        full_response += chunk
            print("\n" + "-" * 50)
            return full_response
        else:
            # éæµå¼è¾“å‡º
            result = response.json()
            print("âœ… å“åº”æˆåŠŸ:")
            print(result.get('response', ''))
            print("-" * 50)
            return result

    except requests.exceptions.Timeout:
        print("âŒ è¯·æ±‚è¶…æ—¶ï¼Œè¯·æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åŠ è½½")
        return None
    except requests.exceptions.RequestException as e:
        print(f"âŒ è¯·æ±‚å¤±è´¥: {e}")
        return None


def call_ollama_chat(api_url, model, messages):
    """
    è°ƒç”¨ Ollama Chat APIï¼ˆå¯¹è¯æ¥å£ï¼‰

    å‚æ•°:
        api_url: API åŸºç¡€åœ°å€
        model: æ¨¡å‹åç§°
        messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨

    è¿”å›:
        å“åº”å†…å®¹
    """
    endpoint = f"{api_url}/api/chat"

    payload = {
        "model": model,
        "messages": messages,
        "stream": False
    }

    try:
        print(f"ğŸš€ æ­£åœ¨è°ƒç”¨ Chat API: {endpoint}")
        print(f"ğŸ’¬ å¯¹è¯æ¶ˆæ¯: {json.dumps(messages, ensure_ascii=False, indent=2)}")
        print("-" * 50)

        response = requests.post(
            endpoint,
            json=payload,
            timeout=300
        )

        response.raise_for_status()
        result = response.json()

        print("âœ… å“åº”æˆåŠŸ:")
        print(result.get('message', {}).get('content', ''))
        print("-" * 50)
        return result

    except requests.exceptions.RequestException as e:
        print(f"âŒ è¯·æ±‚å¤±è´¥: {e}")
        return None


def list_models(api_url):
    """
    åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ¨¡å‹

    å‚æ•°:
        api_url: API åŸºç¡€åœ°å€

    è¿”å›:
        æ¨¡å‹åˆ—è¡¨
    """
    endpoint = f"{api_url}/api/tags"

    try:
        print(f"ğŸ” æŸ¥è¯¢å¯ç”¨æ¨¡å‹: {endpoint}")
        response = requests.get(endpoint, timeout=10)
        response.raise_for_status()

        result = response.json()
        models = result.get('models', [])

        print("ğŸ“š å¯ç”¨æ¨¡å‹åˆ—è¡¨:")
        for model in models:
            print(f"  - {model.get('name')} (å¤§å°: {model.get('size', 0) / 1e9:.2f} GB)")
        print("-" * 50)

        return models

    except requests.exceptions.RequestException as e:
        print(f"âŒ è¯·æ±‚å¤±è´¥: {e}")
        return None


# ===========================
# ä½¿ç”¨ç¤ºä¾‹
# ===========================

def example_local_call():
    """ç¤ºä¾‹ 1: æœ¬åœ°è°ƒç”¨"""
    print("\n" + "=" * 50)
    print("ç¤ºä¾‹ 1: æœ¬åœ° API è°ƒç”¨")
    print("=" * 50 + "\n")

    # åˆ—å‡ºæ¨¡å‹
    list_models(LOCAL_API_URL)

    # Generate API è°ƒç”¨
    call_ollama_generate(
        api_url=LOCAL_API_URL,
        model=MODEL_NAME,
        prompt="ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿè¯·ç”¨100å­—ä»¥å†…å›ç­”ã€‚",
        stream=False
    )

    # Chat API è°ƒç”¨
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„ AI åŠ©æ‰‹ã€‚"},
        {"role": "user", "content": "ä»‹ç»ä¸€ä¸‹ Docker çš„ä¼˜åŠ¿ã€‚"}
    ]
    call_ollama_chat(LOCAL_API_URL, MODEL_NAME, messages)


def example_ngrok_call():
    """ç¤ºä¾‹ 2: é€šè¿‡ ngrok å¤–ç½‘è°ƒç”¨"""
    print("\n" + "=" * 50)
    print("ç¤ºä¾‹ 2: Ngrok å¤–ç½‘ API è°ƒç”¨")
    print("=" * 50 + "\n")

    if NGROK_API_URL == "YOUR_NGROK_URL_HERE":
        print("âš ï¸  è¯·å…ˆè®¾ç½® NGROK_API_URL")
        print("è®¿é—® http://localhost:4040 è·å– ngrok å…¬ç½‘åœ°å€")
        return

    # åˆ—å‡ºæ¨¡å‹
    list_models(NGROK_API_URL)

    # Generate API è°ƒç”¨
    call_ollama_generate(
        api_url=NGROK_API_URL,
        model=MODEL_NAME,
        prompt="è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯ LLMã€‚",
        stream=False
    )


def example_stream_call():
    """ç¤ºä¾‹ 3: æµå¼è¾“å‡ºè°ƒç”¨"""
    print("\n" + "=" * 50)
    print("ç¤ºä¾‹ 3: æµå¼è¾“å‡ºè°ƒç”¨")
    print("=" * 50 + "\n")

    call_ollama_generate(
        api_url=LOCAL_API_URL,
        model=MODEL_NAME,
        prompt="å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„çŸ­è¯—ã€‚",
        stream=True
    )


# ===========================
# cURL å‘½ä»¤ç¤ºä¾‹
# ===========================

def print_curl_examples():
    """æ‰“å° cURL å‘½ä»¤ç¤ºä¾‹"""
    print("\n" + "=" * 50)
    print("cURL å‘½ä»¤ç¤ºä¾‹")
    print("=" * 50 + "\n")

    print("1. åˆ—å‡ºæ‰€æœ‰æ¨¡å‹:")
    print(f'curl {LOCAL_API_URL}/api/tags')
    print()

    print("2. Generate API è°ƒç”¨:")
    print(f'''curl {LOCAL_API_URL}/api/generate -d '{{
  "model": "{MODEL_NAME}",
  "prompt": "Why is the sky blue?",
  "stream": false
}}' ''')
    print()

    print("3. Chat API è°ƒç”¨:")
    print(f'''curl {LOCAL_API_URL}/api/chat -d '{{
  "model": "{MODEL_NAME}",
  "messages": [
    {{"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±ã€‚"}}
  ],
  "stream": false
}}' ''')
    print()

    print("4. é€šè¿‡ Ngrok å¤–ç½‘è°ƒç”¨ï¼ˆæ›¿æ¢ YOUR_NGROK_URLï¼‰:")
    print(f'''curl https://YOUR_NGROK_URL/api/generate -d '{{
  "model": "{MODEL_NAME}",
  "prompt": "Hello from the internet!",
  "stream": false
}}' ''')
    print()


# ===========================
# ä¸»ç¨‹åº
# ===========================

if __name__ == "__main__":
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘     Ollama API è°ƒç”¨ç¤ºä¾‹ç¨‹åº                â•‘
    â•‘     æ”¯æŒæœ¬åœ°å’Œ Ngrok å¤–ç½‘è°ƒç”¨              â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    # è¿è¡Œç¤ºä¾‹
    example_local_call()
    # example_ngrok_call()  # å–æ¶ˆæ³¨é‡Šä»¥æµ‹è¯•å¤–ç½‘è°ƒç”¨
    # example_stream_call()  # å–æ¶ˆæ³¨é‡Šä»¥æµ‹è¯•æµå¼è¾“å‡º

    # æ‰“å° cURL ç¤ºä¾‹
    print_curl_examples()

    print("\nâœ… ç¤ºä¾‹æ‰§è¡Œå®Œæˆï¼")
    print("ğŸ’¡ æç¤º: ä¿®æ”¹ä»£ç ä¸­çš„ NGROK_API_URL åå¯æµ‹è¯•å¤–ç½‘è°ƒç”¨")
