# Ollama + Ngrok ç¦»çº¿ LLM éƒ¨ç½²æ–¹æ¡ˆ

ä½¿ç”¨ Docker éƒ¨ç½² Ollamaï¼Œå¹¶é€šè¿‡ Ngrok å®ç°å¤–ç½‘è®¿é—®çš„å®Œæ•´è§£å†³æ–¹æ¡ˆã€‚

## ğŸ“‹ ç›®å½•ç»“æ„

```
MIAT_offline_llm/
â”œâ”€â”€ docker-compose.yml      # Docker ç¼–æ’é…ç½®
â”œâ”€â”€ .env.example           # ç¯å¢ƒå˜é‡æ¨¡æ¿
â”œâ”€â”€ api_examples.py        # API è°ƒç”¨ç¤ºä¾‹ä»£ç 
â””â”€â”€ README.md             # éƒ¨ç½²è¯´æ˜æ–‡æ¡£
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å‰ç½®è¦æ±‚

1. **Docker & Docker Compose** å·²å®‰è£…
   - Windows: [Docker Desktop](https://www.docker.com/products/docker-desktop/)
   - Linux: `sudo apt-get install docker.io docker-compose`

2. **NVIDIA GPU é©±åŠ¨** (å¦‚æœä½¿ç”¨ GPU)
   - å®‰è£… NVIDIA Docker Runtime: [nvidia-docker](https://github.com/NVIDIA/nvidia-docker)

3. **Ngrok è´¦å·**
   - æ³¨å†Œåœ°å€: https://dashboard.ngrok.com/signup
   - è·å– authtoken: https://dashboard.ngrok.com/get-started/your-authtoken

### æ­¥éª¤ 1: é…ç½®ç¯å¢ƒå˜é‡

```bash
# å¤åˆ¶ç¯å¢ƒå˜é‡æ¨¡æ¿
cp .env.example .env

# ç¼–è¾‘ .env æ–‡ä»¶ï¼Œå¡«å…¥ä½ çš„ ngrok token
# NGROK_AUTHTOKEN=your_actual_token_here
```

### æ­¥éª¤ 2: å¯åŠ¨æœåŠ¡

```bash
# å¯åŠ¨ Ollama å’Œ Ngrok æœåŠ¡
docker-compose up -d

# æŸ¥çœ‹æœåŠ¡çŠ¶æ€
docker-compose ps

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f
```

### æ­¥éª¤ 3: ä¸‹è½½ gpt-oss:20b æ¨¡å‹

æœåŠ¡å¯åŠ¨åï¼Œéœ€è¦è¿›å…¥ Ollama å®¹å™¨ä¸‹è½½æ¨¡å‹ï¼š

```bash
# è¿›å…¥ Ollama å®¹å™¨
docker exec -it ollama_service bash

# ä¸‹è½½ gpt-oss:20b æ¨¡å‹ï¼ˆè¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼‰
ollama pull gpt-oss:20b

# éªŒè¯æ¨¡å‹æ˜¯å¦ä¸‹è½½æˆåŠŸ
ollama list

# é€€å‡ºå®¹å™¨
exit
```

> âš ï¸ **æ³¨æ„**: `gpt-oss:20b` æ¨¡å‹å¤§å°çº¦ 12-15GBï¼Œè¯·ç¡®ä¿æœ‰è¶³å¤Ÿçš„ç£ç›˜ç©ºé—´å’Œç½‘ç»œå¸¦å®½ã€‚

### æ­¥éª¤ 4: è·å– Ngrok å…¬ç½‘åœ°å€

```bash
# æ–¹æ³• 1: è®¿é—® Ngrok Web ç•Œé¢
# æµè§ˆå™¨æ‰“å¼€: http://localhost:4040

# æ–¹æ³• 2: æŸ¥çœ‹æ—¥å¿—è·å– URL
docker-compose logs ngrok | grep "url="
```

ä½ ä¼šçœ‹åˆ°ç±»ä¼¼è¿™æ ·çš„è¾“å‡ºï¼š
```
url=https://xxxx-xx-xx-xxx-xxx.ngrok-free.app
```

è¿™å°±æ˜¯ä½ çš„å…¬ç½‘è®¿é—®åœ°å€ï¼

## ğŸ”§ API è°ƒç”¨ç¤ºä¾‹

### Python è°ƒç”¨ç¤ºä¾‹

å®‰è£…ä¾èµ–ï¼š
```bash
pip install requests
```

è¿è¡Œç¤ºä¾‹ä»£ç ï¼š
```bash
# æœ¬åœ°è°ƒç”¨æµ‹è¯•
python api_examples.py

# ä¿®æ”¹ api_examples.py ä¸­çš„ NGROK_API_URL åæµ‹è¯•å¤–ç½‘è°ƒç”¨
```

### cURL å‘½ä»¤ç¤ºä¾‹

**1. æœ¬åœ°è°ƒç”¨ - Generate API**
```bash
curl http://localhost:11434/api/generate -d '{
  "model": "gpt-oss:20b",
  "prompt": "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
  "stream": false
}'
```

**2. æœ¬åœ°è°ƒç”¨ - Chat API**
```bash
curl http://localhost:11434/api/chat -d '{
  "model": "gpt-oss:20b",
  "messages": [
    {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±ã€‚"}
  ],
  "stream": false
}'
```

**3. å¤–ç½‘è°ƒç”¨ï¼ˆæ›¿æ¢ä¸ºä½ çš„ ngrok URLï¼‰**
```bash
curl https://your-ngrok-url.ngrok-free.app/api/generate -d '{
  "model": "gpt-oss:20b",
  "prompt": "Hello from the internet!",
  "stream": false
}'
```

**4. åˆ—å‡ºæ‰€æœ‰æ¨¡å‹**
```bash
curl http://localhost:11434/api/tags
```

## ğŸ“¡ API ç«¯ç‚¹è¯´æ˜

| ç«¯ç‚¹ | è¯´æ˜ | æ–¹æ³• |
|------|------|------|
| `/api/generate` | æ–‡æœ¬ç”Ÿæˆæ¥å£ | POST |
| `/api/chat` | å¯¹è¯æ¥å£ | POST |
| `/api/tags` | åˆ—å‡ºæ‰€æœ‰æ¨¡å‹ | GET |
| `/api/show` | æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯ | POST |
| `/api/pull` | ä¸‹è½½æ¨¡å‹ | POST |

å®Œæ•´ API æ–‡æ¡£: https://github.com/ollama/ollama/blob/main/docs/api.md

## ğŸŒ å¤–ç½‘è®¿é—®é…ç½®

### ä»å…¶ä»–ç”µè„‘è°ƒç”¨ LLM

1. **è·å– Ngrok URL**
   - è®¿é—® `http://localhost:4040` è·å–å…¬ç½‘åœ°å€
   - æˆ–æŸ¥çœ‹æ—¥å¿—: `docker-compose logs ngrok`

2. **ä½¿ç”¨å…¬ç½‘åœ°å€è°ƒç”¨**
   ```python
   import requests

   NGROK_URL = "https://your-url.ngrok-free.app"

   response = requests.post(
       f"{NGROK_URL}/api/generate",
       json={
           "model": "gpt-oss:20b",
           "prompt": "Hello from internet!",
           "stream": False
       }
   )

   print(response.json()['response'])
   ```

3. **åœ¨å…¶ä»–è®¾å¤‡æµ‹è¯•**
   - ä½¿ç”¨ç›¸åŒçš„ ngrok URL å³å¯ä»ä»»ä½•è”ç½‘è®¾å¤‡è®¿é—®

### Ngrok å…è´¹ç‰ˆé™åˆ¶

- âœ… HTTP/HTTPS éš§é“
- âœ… éšæœºå­åŸŸå
- âš ï¸ è¿æ¥é™åˆ¶: 40 è¿æ¥/åˆ†é’Ÿ
- âš ï¸ éš§é“ä¼šè¯æ—¶é—´: 8å°æ—¶åéœ€é‡å¯

å¦‚éœ€æ›´ç¨³å®šçš„æœåŠ¡ï¼Œå»ºè®®å‡çº§ Ngrok ä»˜è´¹è®¡åˆ’æˆ–ä½¿ç”¨å…¶ä»–å†…ç½‘ç©¿é€æ–¹æ¡ˆã€‚

## ğŸ› ï¸ å¸¸ç”¨å‘½ä»¤

### æœåŠ¡ç®¡ç†

```bash
# å¯åŠ¨æœåŠ¡
docker-compose up -d

# åœæ­¢æœåŠ¡
docker-compose down

# é‡å¯æœåŠ¡
docker-compose restart

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f ollama
docker-compose logs -f ngrok

# æŸ¥çœ‹èµ„æºä½¿ç”¨
docker stats
```

### æ¨¡å‹ç®¡ç†

```bash
# è¿›å…¥å®¹å™¨
docker exec -it ollama_service bash

# åˆ—å‡ºå·²å®‰è£…æ¨¡å‹
ollama list

# ä¸‹è½½æ–°æ¨¡å‹
ollama pull <model-name>

# åˆ é™¤æ¨¡å‹
ollama rm <model-name>

# è¿è¡Œæ¨¡å‹ï¼ˆäº¤äº’å¼ï¼‰
ollama run gpt-oss:20b
```

### æ•°æ®æŒä¹…åŒ–

æ•°æ®ä¿å­˜åœ¨ Docker volumes ä¸­ï¼š
- `ollama_data`: å­˜å‚¨æ¨¡å‹æ–‡ä»¶
- `ngrok_data`: å­˜å‚¨ ngrok é…ç½®

```bash
# æŸ¥çœ‹ volumes
docker volume ls

# å¤‡ä»½ volume
docker run --rm -v ollama_data:/data -v $(pwd):/backup ubuntu tar czf /backup/ollama_backup.tar.gz /data

# æ¸…ç†æœªä½¿ç”¨çš„ volumes
docker volume prune
```

## ğŸ” æ•…éšœæ’é™¤

### é—®é¢˜ 1: Ollama æœåŠ¡æ— æ³•å¯åŠ¨

**å¯èƒ½åŸå› **: GPU é©±åŠ¨æœªå®‰è£…

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥ NVIDIA é©±åŠ¨
nvidia-smi

# å¦‚æœæ—  GPUï¼Œä¿®æ”¹ docker-compose.ymlï¼Œç§»é™¤ GPU é…ç½®
# æ³¨é‡Šæ‰ä»¥ä¸‹éƒ¨åˆ†:
# deploy:
#   resources:
#     reservations:
#       devices:
#         - driver: nvidia
#           count: all
#           capabilities: [gpu]
```

### é—®é¢˜ 2: Ngrok æ— æ³•è¿æ¥

**å¯èƒ½åŸå› **: Token æœªæ­£ç¡®é…ç½®

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥ .env æ–‡ä»¶
cat .env

# ç¡®ä¿ NGROK_AUTHTOKEN å·²è®¾ç½®
# é‡å¯æœåŠ¡
docker-compose restart ngrok
```

### é—®é¢˜ 3: æ¨¡å‹å“åº”ç¼“æ…¢

**å¯èƒ½åŸå› **:
- æ¨¡å‹å¤ªå¤§ï¼Œå†…å­˜ä¸è¶³
- CPU æ¨¡å¼è¿è¡Œï¼ˆæœªä½¿ç”¨ GPUï¼‰

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥èµ„æºä½¿ç”¨
docker stats

# ä½¿ç”¨æ›´å°çš„æ¨¡å‹
ollama pull gpt-oss:7b  # æ›´å°çš„ç‰ˆæœ¬

# ç¡®è®¤ GPU æ˜¯å¦è¢«ä½¿ç”¨
docker exec -it ollama_service nvidia-smi
```

### é—®é¢˜ 4: API è°ƒç”¨è¶…æ—¶

**å¯èƒ½åŸå› **: é¦–æ¬¡è°ƒç”¨éœ€è¦åŠ è½½æ¨¡å‹åˆ°å†…å­˜

**è§£å†³æ–¹æ¡ˆ**:
- å¢åŠ è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆå»ºè®® 300 ç§’ï¼‰
- é¢„çƒ­æ¨¡å‹: `docker exec -it ollama_service ollama run gpt-oss:20b "test"`

### é—®é¢˜ 5: Ngrok è¿æ¥æ–­å¼€

**åŸå› **: å…è´¹ç‰ˆ ngrok éš§é“ä¼šè¯é™åˆ¶ï¼ˆ8å°æ—¶ï¼‰

**è§£å†³æ–¹æ¡ˆ**:
```bash
# é‡å¯ ngrok æœåŠ¡
docker-compose restart ngrok

# è·å–æ–°çš„ URL
docker-compose logs ngrok | grep "url="
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. GPU åŠ é€Ÿ

ç¡®ä¿ä½¿ç”¨ NVIDIA GPU è¿è¡Œï¼š
```bash
# éªŒè¯ GPU å¯ç”¨
docker exec -it ollama_service nvidia-smi

# æŸ¥çœ‹ GPU ä½¿ç”¨æƒ…å†µ
watch -n 1 nvidia-smi
```

### 2. å†…å­˜ä¼˜åŒ–

å¦‚æœå†…å­˜ä¸è¶³ï¼Œå¯ä»¥é…ç½® Ollama å‚æ•°ï¼š
```yaml
# åœ¨ docker-compose.yml ä¸­æ·»åŠ ç¯å¢ƒå˜é‡
environment:
  - OLLAMA_NUM_PARALLEL=1
  - OLLAMA_MAX_LOADED_MODELS=1
```

### 3. ç½‘ç»œä¼˜åŒ–

ä½¿ç”¨ Ngrok æ—¶å¯èƒ½é‡åˆ°å»¶è¿Ÿï¼Œä¼˜åŒ–å»ºè®®ï¼š
- é€‰æ‹©ç¦»ä½ æœ€è¿‘çš„ Ngrok æœåŠ¡å™¨åŒºåŸŸ
- è€ƒè™‘å‡çº§ Ngrok ä»˜è´¹è®¡åˆ’
- æˆ–ä½¿ç”¨å…¶ä»–å†…ç½‘ç©¿é€å·¥å…·ï¼ˆfrp, cloudflare tunnelï¼‰

## ğŸ” å®‰å…¨å»ºè®®

1. **ä¸è¦å…¬å¼€åˆ†äº« Ngrok URL**
   - URL æš´éœ²åä»»ä½•äººéƒ½å¯ä»¥è®¿é—®ä½ çš„ LLM

2. **æ·»åŠ è®¤è¯**
   - è€ƒè™‘åœ¨ API å‰æ·»åŠ è®¤è¯å±‚ï¼ˆnginx + basic authï¼‰

3. **ç›‘æ§ä½¿ç”¨é‡**
   - å®šæœŸæ£€æŸ¥ Ollama æ—¥å¿—
   - è®¾ç½® ngrok è®¿é—®é™åˆ¶

4. **ç¯å¢ƒå˜é‡å®‰å…¨**
   - ä¸è¦æäº¤ `.env` åˆ° git
   - `.env` å·²åŒ…å«åœ¨ `.gitignore` ä¸­

## ğŸ“š ç›¸å…³èµ„æº

- [Ollama å®˜æ–¹æ–‡æ¡£](https://github.com/ollama/ollama)
- [Ollama API æ–‡æ¡£](https://github.com/ollama/ollama/blob/main/docs/api.md)
- [Ngrok æ–‡æ¡£](https://ngrok.com/docs)
- [Docker Compose æ–‡æ¡£](https://docs.docker.com/compose/)

## ğŸ†˜ è·å–å¸®åŠ©

é‡åˆ°é—®é¢˜ï¼Ÿ
1. æŸ¥çœ‹æ—¥å¿—: `docker-compose logs`
2. æ£€æŸ¥æœåŠ¡çŠ¶æ€: `docker-compose ps`
3. è®¿é—® Ollama GitHub Issues: https://github.com/ollama/ollama/issues

## ğŸ“ è®¸å¯è¯

æœ¬é¡¹ç›®ä»…ä¾›å­¦ä¹ å’Œä¸ªäººä½¿ç”¨ã€‚

---

**ç¥ä½ éƒ¨ç½²é¡ºåˆ©ï¼ğŸ‰**
