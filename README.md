# Ollama å†…ç½‘ LLM éƒ¨ç½²æ–¹æ¡ˆ

ä½¿ç”¨ Docker éƒ¨ç½² Ollamaï¼Œæ”¯æŒå†…ç½‘è®¾å¤‡è®¿é—®çš„ç¦»çº¿ LLM è§£å†³æ–¹æ¡ˆã€‚

## ğŸ“‹ ç›®å½•ç»“æ„

```
MIAT_offline_llm/
â”œâ”€â”€ docker-compose.yml      # Docker ç¼–æ’é…ç½®
â”œâ”€â”€ api_examples.py         # API è°ƒç”¨ç¤ºä¾‹ä»£ç 
â””â”€â”€ README.md              # éƒ¨ç½²è¯´æ˜æ–‡æ¡£
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å‰ç½®è¦æ±‚

1. **Docker & Docker Compose** å·²å®‰è£…
   - Windows: [Docker Desktop](https://www.docker.com/products/docker-desktop/)
   - Linux: `sudo apt-get install docker.io docker-compose`

2. **NVIDIA GPU é©±åŠ¨**ï¼ˆå¯é€‰ï¼Œä½¿ç”¨ GPU åŠ é€Ÿï¼‰
   - å®‰è£… NVIDIA Docker Runtime: [nvidia-docker](https://github.com/NVIDIA/nvidia-docker)

### æ­¥éª¤ 1: å¯åŠ¨æœåŠ¡

```bash
# å¯åŠ¨ Ollama æœåŠ¡
docker-compose up -d

# æŸ¥çœ‹æœåŠ¡çŠ¶æ€
docker-compose ps

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f
```

### æ­¥éª¤ 2: ä¸‹è½½ gpt-oss:20b æ¨¡å‹

æœåŠ¡å¯åŠ¨åï¼Œéœ€è¦è¿›å…¥ Ollama å®¹å™¨ä¸‹è½½æ¨¡å‹ï¼š

```bash
# è¿›å…¥ Ollama å®¹å™¨
docker exec -it ollama_service bash

# ä¸‹è½½ gpt-oss:20b æ¨¡å‹ï¼ˆè¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼‰
ollama pull gpt-oss:20b

# éªŒè¯æ¨¡å‹æ˜¯å¦ä¸‹è½½æˆåŠŸ
ollama list

# é€€å‡ºå®¹å™¨
exit
```

> âš ï¸ **æ³¨æ„**: `gpt-oss:20b` æ¨¡å‹å¤§å°çº¦ 12-15GBï¼Œè¯·ç¡®ä¿æœ‰è¶³å¤Ÿçš„ç£ç›˜ç©ºé—´å’Œç½‘ç»œå¸¦å®½ã€‚

### æ­¥éª¤ 3: è·å–å†…ç½‘ IP åœ°å€

```bash
# Windows
ipconfig

# Linux/Mac
ifconfig
# æˆ–
ip addr show
```

æ‰¾åˆ°ä½ çš„å†…ç½‘ IP åœ°å€ï¼Œé€šå¸¸æ˜¯ `192.168.x.x` æˆ– `10.x.x.x` æ ¼å¼ã€‚

## ğŸ”§ API è°ƒç”¨ç¤ºä¾‹

### Python è°ƒç”¨ç¤ºä¾‹

å®‰è£…ä¾èµ–ï¼š
```bash
pip install requests
```

è¿è¡Œç¤ºä¾‹ä»£ç ï¼š
```bash
# æœ¬åœ°è°ƒç”¨æµ‹è¯•
python api_examples.py
```

ç¤ºä¾‹ä»£ç ä¼šè‡ªåŠ¨æ˜¾ç¤ºä½ çš„å†…ç½‘ IP å’Œè®¿é—®åœ°å€ã€‚

### æ‰‹åŠ¨è°ƒç”¨ç¤ºä¾‹

**æœ¬åœ°è°ƒç”¨**:
```python
import requests

response = requests.post(
    "http://localhost:11434/api/generate",
    json={
        "model": "gpt-oss:20b",
        "prompt": "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
        "stream": False
    }
)

print(response.json()['response'])
```

**ä»å…¶ä»–å†…ç½‘è®¾å¤‡è°ƒç”¨**ï¼ˆæ›¿æ¢ `192.168.1.100` ä¸ºå®é™…æœåŠ¡å™¨ IPï¼‰:
```python
import requests

response = requests.post(
    "http://192.168.1.100:11434/api/generate",
    json={
        "model": "gpt-oss:20b",
        "prompt": "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
        "stream": False
    }
)

print(response.json()['response'])
```

### cURL å‘½ä»¤ç¤ºä¾‹

**1. æœ¬åœ°è°ƒç”¨ - Generate API**
```bash
curl http://localhost:11434/api/generate -d '{
  "model": "gpt-oss:20b",
  "prompt": "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
  "stream": false
}'
```

**2. æœ¬åœ°è°ƒç”¨ - Chat API**
```bash
curl http://localhost:11434/api/chat -d '{
  "model": "gpt-oss:20b",
  "messages": [
    {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±ã€‚"}
  ],
  "stream": false
}'
```

**3. å†…ç½‘è°ƒç”¨ï¼ˆæ›¿æ¢ä¸ºå®é™…æœåŠ¡å™¨ IPï¼‰**
```bash
curl http://192.168.1.100:11434/api/generate -d '{
  "model": "gpt-oss:20b",
  "prompt": "Hello from intranet!",
  "stream": false
}'
```

**4. åˆ—å‡ºæ‰€æœ‰æ¨¡å‹**
```bash
curl http://localhost:11434/api/tags
```

## ğŸ“¡ API ç«¯ç‚¹è¯´æ˜

| ç«¯ç‚¹ | è¯´æ˜ | æ–¹æ³• |
|------|------|------|
| `/api/generate` | æ–‡æœ¬ç”Ÿæˆæ¥å£ | POST |
| `/api/chat` | å¯¹è¯æ¥å£ | POST |
| `/api/tags` | åˆ—å‡ºæ‰€æœ‰æ¨¡å‹ | GET |
| `/api/show` | æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯ | POST |
| `/api/pull` | ä¸‹è½½æ¨¡å‹ | POST |

å®Œæ•´ API æ–‡æ¡£: https://github.com/ollama/ollama/blob/main/docs/api.md

## ğŸŒ å†…ç½‘è®¿é—®é…ç½®

### ä»å…¶ä»–è®¾å¤‡è®¿é—® LLM

1. **ç¡®è®¤æœåŠ¡å™¨å†…ç½‘ IP**
   - å‡è®¾æœåŠ¡å™¨ IP æ˜¯ `192.168.1.100`

2. **ç¡®ä¿é˜²ç«å¢™å¼€æ”¾ç«¯å£**

   **Windows é˜²ç«å¢™**:
   ```powershell
   # PowerShellï¼ˆç®¡ç†å‘˜æƒé™ï¼‰
   New-NetFirewallRule -DisplayName "Ollama API" -Direction Inbound -LocalPort 11434 -Protocol TCP -Action Allow
   ```

   **Linux é˜²ç«å¢™**:
   ```bash
   # ufw
   sudo ufw allow 11434/tcp

   # firewalld
   sudo firewall-cmd --permanent --add-port=11434/tcp
   sudo firewall-cmd --reload
   ```

3. **ä»å…¶ä»–è®¾å¤‡æµ‹è¯•è¿æ¥**
   ```bash
   # æµ‹è¯•è¿æ¥
   curl http://192.168.1.100:11434/api/tags
   ```

### å†…ç½‘è®¾å¤‡é…ç½®ç¤ºä¾‹

**ä»æ‰‹æœº/å¹³æ¿è®¿é—®**:
- ç¡®ä¿è®¾å¤‡è¿æ¥åˆ°åŒä¸€ä¸ªå†…ç½‘ï¼ˆåŒä¸€ä¸ª Wi-Fiï¼‰
- ä½¿ç”¨æµè§ˆå™¨è®¿é—®: `http://192.168.1.100:11434/api/tags`

**ä»å…¶ä»–ç”µè„‘è®¿é—®**:
```python
import requests

SERVER_IP = "192.168.1.100"
API_URL = f"http://{SERVER_IP}:11434"

# æµ‹è¯•è¿æ¥
response = requests.get(f"{API_URL}/api/tags")
print(response.json())
```

## ğŸ› ï¸ å¸¸ç”¨å‘½ä»¤

### æœåŠ¡ç®¡ç†

```bash
# å¯åŠ¨æœåŠ¡
docker-compose up -d

# åœæ­¢æœåŠ¡
docker-compose down

# é‡å¯æœåŠ¡
docker-compose restart

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f ollama

# æŸ¥çœ‹èµ„æºä½¿ç”¨
docker stats
```

### æ¨¡å‹ç®¡ç†

```bash
# è¿›å…¥å®¹å™¨
docker exec -it ollama_service bash

# åˆ—å‡ºå·²å®‰è£…æ¨¡å‹
ollama list

# ä¸‹è½½æ–°æ¨¡å‹
ollama pull <model-name>

# åˆ é™¤æ¨¡å‹
ollama rm <model-name>

# è¿è¡Œæ¨¡å‹ï¼ˆäº¤äº’å¼ï¼‰
ollama run gpt-oss:20b
```

### æ•°æ®æŒä¹…åŒ–

æ•°æ®ä¿å­˜åœ¨ Docker volume ä¸­ï¼š
- `ollama_data`: å­˜å‚¨æ¨¡å‹æ–‡ä»¶

```bash
# æŸ¥çœ‹ volumes
docker volume ls

# å¤‡ä»½ volume
docker run --rm -v ollama_data:/data -v $(pwd):/backup ubuntu tar czf /backup/ollama_backup.tar.gz /data

# æ¸…ç†æœªä½¿ç”¨çš„ volumes
docker volume prune
```

## ğŸ” æ•…éšœæ’é™¤

### é—®é¢˜ 1: Ollama æœåŠ¡æ— æ³•å¯åŠ¨

**å¯èƒ½åŸå› **: GPU é©±åŠ¨æœªå®‰è£…

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥ NVIDIA é©±åŠ¨
nvidia-smi

# å¦‚æœæ—  GPUï¼Œä¿®æ”¹ docker-compose.ymlï¼Œç§»é™¤ GPU é…ç½®
# æ³¨é‡Šæ‰ä»¥ä¸‹éƒ¨åˆ†:
# deploy:
#   resources:
#     reservations:
#       devices:
#         - driver: nvidia
#           count: all
#           capabilities: [gpu]
```

### é—®é¢˜ 2: å†…ç½‘è®¾å¤‡æ— æ³•è®¿é—®

**å¯èƒ½åŸå› **: é˜²ç«å¢™é˜»æ­¢æˆ–ç½‘ç»œé…ç½®é—®é¢˜

**è§£å†³æ–¹æ¡ˆ**:
```bash
# 1. æ£€æŸ¥æœåŠ¡æ˜¯å¦è¿è¡Œ
docker-compose ps

# 2. æ£€æŸ¥ç«¯å£æ˜¯å¦å¼€æ”¾
netstat -an | grep 11434

# 3. åœ¨æœåŠ¡å™¨ä¸Šæµ‹è¯•æœ¬åœ°è®¿é—®
curl http://localhost:11434/api/tags

# 4. æ£€æŸ¥é˜²ç«å¢™
# Windows: æ§åˆ¶é¢æ¿ -> Windows Defender é˜²ç«å¢™ -> é«˜çº§è®¾ç½®
# Linux: sudo ufw status
```

### é—®é¢˜ 3: æ¨¡å‹å“åº”ç¼“æ…¢

**å¯èƒ½åŸå› **:
- æ¨¡å‹å¤ªå¤§ï¼Œå†…å­˜ä¸è¶³
- CPU æ¨¡å¼è¿è¡Œï¼ˆæœªä½¿ç”¨ GPUï¼‰

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥èµ„æºä½¿ç”¨
docker stats

# ä½¿ç”¨æ›´å°çš„æ¨¡å‹
ollama pull gpt-oss:7b  # æ›´å°çš„ç‰ˆæœ¬

# ç¡®è®¤ GPU æ˜¯å¦è¢«ä½¿ç”¨
docker exec -it ollama_service nvidia-smi
```

### é—®é¢˜ 4: API è°ƒç”¨è¶…æ—¶

**å¯èƒ½åŸå› **: é¦–æ¬¡è°ƒç”¨éœ€è¦åŠ è½½æ¨¡å‹åˆ°å†…å­˜

**è§£å†³æ–¹æ¡ˆ**:
- å¢åŠ è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆå»ºè®® 300 ç§’ï¼‰
- é¢„çƒ­æ¨¡å‹: `docker exec -it ollama_service ollama run gpt-oss:20b "test"`

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. GPU åŠ é€Ÿ

ç¡®ä¿ä½¿ç”¨ NVIDIA GPU è¿è¡Œï¼š
```bash
# éªŒè¯ GPU å¯ç”¨
docker exec -it ollama_service nvidia-smi

# æŸ¥çœ‹ GPU ä½¿ç”¨æƒ…å†µ
watch -n 1 nvidia-smi
```

### 2. å†…å­˜ä¼˜åŒ–

å¦‚æœå†…å­˜ä¸è¶³ï¼Œå¯ä»¥é…ç½® Ollama å‚æ•°ï¼š
```yaml
# åœ¨ docker-compose.yml ä¸­æ·»åŠ ç¯å¢ƒå˜é‡
environment:
  - OLLAMA_NUM_PARALLEL=1
  - OLLAMA_MAX_LOADED_MODELS=1
```

### 3. ç½‘ç»œä¼˜åŒ–

**ä¼˜åŒ–å†…ç½‘è®¿é—®é€Ÿåº¦**:
- ä½¿ç”¨æœ‰çº¿è¿æ¥è€Œé Wi-Fi
- ç¡®ä¿è·¯ç”±å™¨æ€§èƒ½è¶³å¤Ÿ
- è€ƒè™‘ä½¿ç”¨åƒå…†ç½‘ç»œäº¤æ¢æœº

## ğŸ” å®‰å…¨å»ºè®®

1. **å†…ç½‘è®¿é—®æ§åˆ¶**
   - åªåœ¨å—ä¿¡ä»»çš„å†…ç½‘ä¸­å¼€æ”¾æœåŠ¡
   - ä¸è¦å°†ç«¯å£æš´éœ²åˆ°å…¬ç½‘

2. **æ·»åŠ è®¤è¯**ï¼ˆå¯é€‰ï¼‰
   - è€ƒè™‘åœ¨ API å‰æ·»åŠ è®¤è¯å±‚ï¼ˆnginx + basic authï¼‰
   - æˆ–ä½¿ç”¨ VPN è®¿é—®å†…ç½‘

3. **ç›‘æ§ä½¿ç”¨é‡**
   - å®šæœŸæ£€æŸ¥ Ollama æ—¥å¿—
   - ç›‘æ§å¼‚å¸¸è¯·æ±‚

4. **é˜²ç«å¢™é…ç½®**
   - åªå¼€æ”¾å¿…è¦çš„ç«¯å£ï¼ˆ11434ï¼‰
   - é™åˆ¶è®¿é—®æ¥æº IP èŒƒå›´

## ğŸ“± ç§»åŠ¨è®¾å¤‡è®¿é—®ç¤ºä¾‹

### Android/iOS åº”ç”¨ç¤ºä¾‹

å¯ä»¥ä½¿ç”¨ä»»ä½•æ”¯æŒ HTTP è¯·æ±‚çš„åº”ç”¨æˆ–è‡ªå·±å¼€å‘ï¼š

```javascript
// JavaScript/React Native ç¤ºä¾‹
const SERVER_IP = "192.168.1.100";

fetch(`http://${SERVER_IP}:11434/api/generate`, {
  method: 'POST',
  headers: {
    'Content-Type': 'application/json',
  },
  body: JSON.stringify({
    model: 'gpt-oss:20b',
    prompt: 'ä½ å¥½',
    stream: false
  })
})
.then(response => response.json())
.then(data => console.log(data.response));
```

## ğŸ“š ç›¸å…³èµ„æº

- [Ollama å®˜æ–¹æ–‡æ¡£](https://github.com/ollama/ollama)
- [Ollama API æ–‡æ¡£](https://github.com/ollama/ollama/blob/main/docs/api.md)
- [Docker Compose æ–‡æ¡£](https://docs.docker.com/compose/)

## ğŸ†˜ è·å–å¸®åŠ©

é‡åˆ°é—®é¢˜ï¼Ÿ
1. æŸ¥çœ‹æ—¥å¿—: `docker-compose logs`
2. æ£€æŸ¥æœåŠ¡çŠ¶æ€: `docker-compose ps`
3. è®¿é—® Ollama GitHub Issues: https://github.com/ollama/ollama/issues

## ğŸ“ è®¸å¯è¯

æœ¬é¡¹ç›®ä»…ä¾›å­¦ä¹ å’Œä¸ªäººä½¿ç”¨ã€‚

---

**ç¥ä½ éƒ¨ç½²é¡ºåˆ©ï¼ğŸ‰**
